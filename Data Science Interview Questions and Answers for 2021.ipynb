{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Science Interview Questions and Answers for 2021**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **General Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What are the differences between supervised and unsupervised learning?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised Learning | Unsupervised Learning\n",
    "-------------------- | --------------------- \n",
    "**Uses known and labeled data as input** | **Uses unlabeled data as input** \n",
    "**Supervised learning has a feedback mechanism**  | **Unsupervised learning has no feedback mechanism**\n",
    "**The most commonly used supervised learning algorithms are decision trees, logistic regression, and support vector machine** |  **The most commonly used unsupervised learning algorithms are k-means clustering, hierarchical clustering, and apriori algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What is logistic regression?**\n",
    "\n",
    "Logistic regression is also known as the logit model. It is a technique used to forecast the binary outcome from a linear combination of predictor variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. How is logistic regression done?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression measures the relationship between the dependent variable (our label of what we want to predict) and one or more independent variables (our features) by estimating probability using its underlying logistic function (sigmoid)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Explain the steps in making a decision tree.**\n",
    "\n",
    "* Take the entire data set as input.\n",
    "* Look for a split that maximizes the separation of the classes. A split is any test that divides the data into two sets.\n",
    "* Apply the split to the input data (divide step).\n",
    "* Re-apply steps one and two to the divided data.\n",
    "* Stop when you meet any stopping criteria.\n",
    "\n",
    "This step is called pruning. Clean up the tree if you went too far doing splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. What are the feature vectors?**\n",
    "\n",
    "A feature vector is an n-dimensional vector of numerical features that represent an object. In machine learning, feature vectors are used to represent numeric or symbolic characteristics (called features) of an object in a mathematical way that's easy to analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. What are recommender systems?**\n",
    "\n",
    "Recommender systems are a subclass of information filtering systems that are meant to predict the preferences or ratings that a user would give to a product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Explain cross-validation.**\n",
    "\n",
    "Cross-validation is a model validation technique for evaluating how the outcomes of a statistical analysis will generalize to an independent data set. It is mainly used in backgrounds where the objective is to forecast and one wants to estimate how accurately a model will accomplish in practice. \n",
    "\n",
    "\n",
    "The goal of cross-validation is to term a data set to test the model in the training phase (i.e. validation data set) to limit problems like overfitting and gain insight into how the model will generalize to an independent data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. What is collaborative filtering?**\n",
    "\n",
    "Most recommender systems use this filtering process to find patterns and information by collaborating perspectives, numerous data sources, and several agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Does gradient descent methods always converge to similar points?**\n",
    "\n",
    "They do not, because in some cases, they reach a local minima or a local optima point. You would not reach the global optima point. This is governed by the data and the starting conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. What is the goal of A/B Testing?**\n",
    "\n",
    "This is statistical hypothesis testing for randomized experiments with two variables, A and B. The objective of A/B testing is to detect any changes to a web page to maximize or increase the outcome of a strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11. What are the drawbacks of the linear model?**\n",
    "\n",
    "* The assumption of linearity of the errors\n",
    "* It can't be used for count outcomes or binary outcomes\n",
    "* There are overfitting problems that it can't solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12. What is the law of large numbers?**\n",
    "\n",
    "It is a theorem that describes the result of performing the same experiment very frequently. This theorem forms the basis of frequency-style thinking. It states that the sample mean, sample variance, and sample standard deviation converge to what they are trying to estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**13. What are the confounding variables?** \n",
    "\n",
    "These are extraneous variables in a statistical model that correlates directly or inversely with both the dependent and the independent variable. The estimate fails to account for the confounding factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**14. What is star schema?**\n",
    "\n",
    "It is a traditional database schema with a central table. Satellite tables map IDs to physical names or descriptions and can be connected to the central fact table using the ID fields; these tables are known as lookup tables and are principally useful in real-time applications, as they save a lot of memory. Sometimes, star schemas involve several layers of summarization to recover information faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**15. How regularly must an algorithm be updated?**\n",
    "\n",
    "You will want to update an algorithm when:\n",
    "\n",
    "You want the model to evolve as data streams through infrastructure\n",
    "The underlying data source is changing\n",
    "There is a case of non-stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**16. What are eigenvalue and eigenvector?**\n",
    "\n",
    "Eigenvalues are the directions along which a particular linear transformation acts by flipping, compressing, or stretching.\n",
    "\n",
    "Eigenvectors are for understanding linear transformations. In data analysis, we usually calculate the eigenvectors for a correlation or covariance matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How will you calculate eigenvalues and eigenvectors of the following 3x3 matrix?**\n",
    "\n",
    "\n",
    "-2 | -4 | 2\n",
    "--- | ------ | ------\n",
    "\n",
    "-2 | 1 | 2\n",
    "--- | ------ | ------\n",
    "\n",
    "4 | 2 | 5\n",
    "--- | ------ | ------ \n",
    "\n",
    "\n",
    "The characteristic equation is as shown:\n",
    "\n",
    "Expanding determinant:\n",
    "\n",
    "(-2 – λ) [(1-λ) (5-λ)-2x2] + 4[(-2) x (5-λ) -4x2] + 2[(-2) x 2-4(1-λ)] =0\n",
    "\n",
    "- λ3 + 4λ2 + 27λ – 90 = 0,\n",
    "\n",
    "λ3 - 4 λ2 -27 λ + 90 = 0\n",
    "\n",
    "Here we have an algebraic equation built from the eigenvectors.\n",
    "\n",
    "By hit and trial:\n",
    "\n",
    "33 – 4 x 32 - 27 x 3 +90 = 0\n",
    "\n",
    "Hence, (λ - 3) is a factor:\n",
    "\n",
    "λ3 - 4 λ2 - 27 λ +90 = (λ – 3) (λ2 – λ – 30)\n",
    "\n",
    "Eigenvalues are 3,-5,6:\n",
    "\n",
    "(λ – 3) (λ2 – λ – 30) = (λ – 3) (λ+5) (λ-6),\n",
    "\n",
    "Calculate eigenvector for λ = 3\n",
    "\n",
    "For X = 1,\n",
    "\n",
    "-5 - 4Y + 2Z =0,\n",
    "\n",
    "-2 - 2Y + 2Z =0\n",
    "\n",
    "Subtracting the two equations: \n",
    "\n",
    "3 + 2Y = 0,\n",
    "\n",
    "Subtracting back into second equation:\n",
    "\n",
    "Y = -(3/2) \n",
    "\n",
    "Z = -(1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**17. Why is resampling done?**\n",
    "\n",
    "Resampling is done in any of these cases:\n",
    "\n",
    "Estimating the accuracy of sample statistics by using subsets of accessible data, or drawing randomly with replacement from a set of data points\n",
    "Substituting labels on data points when performing significance tests\n",
    "Validating models by using random subsets (bootstrapping, cross-validation)\n",
    "\n",
    "\n",
    "**NB:**\n",
    "- Resampling is the method that consists of drawing repeated samples from the original data samples. The method of Resampling is a nonparametric method of statistical inference. In other words, the method of resampling does not involve the utilization of the generic distribution tables (for example, normal distribution tables) in order to compute approximate p probability values.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**18. What is selection bias?**\n",
    "\n",
    "Selection bias, in general, is a problematic situation in which error is introduced due to a non-random population sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**19. What are the types of biases that can occur during sampling?**\n",
    "\n",
    "* Selection bias\n",
    "* Undercoverage bias\n",
    "* Survivorship bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**20. What is survivorship bias?**\n",
    "\n",
    "Survivorship bias is the logical error of focusing on aspects that support surviving a process and casually overlooking those that did not because of their lack of prominence. This can lead to wrong conclusions in numerous ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**21. How do you build a random forest model?**\n",
    "\n",
    "A random forest is built up of a number of decision trees. If you split the data into different packages and make a decision tree in each of the different groups of data, the random forest brings all those trees together. \n",
    "\n",
    "**Steps to build a random forest model:**\n",
    "- Randomly select 'k' features from a total of 'm' features where k << m\n",
    "- Among the 'k' features, calculate the node D using the best split point\n",
    "- Split the node into daughter nodes using the best split\n",
    "- Repeat steps two and three until leaf nodes are finalized \n",
    "- Build forest by repeating steps one to four for 'n' times to create 'n' number of trees "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**22. Which of the following machine learning algorithms can be used for inputting missing values of both categorical and continuous variables?**\n",
    "\n",
    "- K-means clustering\n",
    "- Linear regression \n",
    "- K-NN (k-nearest neighbor)\n",
    "- Decision trees \n",
    "\n",
    "The K nearest neighbor algorithm can be used because it can compute the nearest neighbor and if it doesn't have a value, it just computes the nearest neighbor based on all the other features. \n",
    "\n",
    "When you're dealing with K-means clustering or linear regression, you need to do that in your pre-processing, otherwise, they'll crash. Decision trees also have the same problem, although there is some variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**23. You are given a data set consisting of variables with more than 30 percent missing values. How will you deal with them?** \n",
    "\n",
    "The following are ways to handle missing data values:\n",
    "\n",
    "- If the data set is large, we can just simply remove the rows with missing data values. It is the quickest way; we use the rest of the data to predict the values.\n",
    "\n",
    "- For smaller data sets, we can substitute missing values with the mean or average of the rest of the data using the pandas' data frame in python. There are different ways to do so, such as df.mean(), df.fillna(mean)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**24.  For the given points, how will you calculate the Euclidean distance in Python?**\n",
    "\n",
    "plot1 = [1,3]\n",
    "\n",
    "plot2 = [2,5]\n",
    "\n",
    "\n",
    "The Euclidean distance can be calculated as follows:\n",
    "\n",
    "euclidean_distance = sqrt( (plot1[0]-plot2[0])**2 + (plot1[1]-plot2[1])**2 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**25. What are dimensionality reduction and its benefits?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction refers to the process of converting a data set with vast dimensions into data with fewer dimensions (fields) to convey similar information concisely. \n",
    "\n",
    "This reduction helps in compressing data and reducing storage space. It also reduces computation time as fewer dimensions lead to less computing. It removes redundant features; for example, there's no point in storing a value in two different units (meters and inches). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
